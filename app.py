import subprocess
import json
import os
import streamlit as st
import pandas as pd
import re
import sqlite3

from pathlib import Path
from working_DB.initial_scan import scan_folder_and_store
from metadata.Magic_Scan import run_magic_numbers_on_db
from forensic.crude_benefits import analyze_duplicates
from forensic.Benford_distrib import analyze_benford_distribution
from metadata.metadata_router import run_global_metadata_population
from analytics.volume_map import get_folder_volume_df
from forensic.anomalies import run_forensic_analysis
from src.config import DB_PATH          # <-- import du chemin calcul√© dynamiquement
from src.forensic_detector import run_forensic_scan

if 'selected_tool' not in st.session_state:
    st.session_state['selected_tool'] = None

st.title("Locard.IA AMC 0.2")
st.sidebar.title("üõ†Ô∏è Tools")


# ---------------------------
# --- FOLDER TO WORK WITH ---
# ---------------------------

folder_toscan = st.text_input(
    "üìÅ Absolute path to root folder",
    placeholder="/Users/nom/Documents/mon_dossier"
)

if st.button("Confirm."):
    if folder_toscan and Path(folder_toscan).exists():
        # Sauvegarder dans session_state
        st.session_state['dossier_cible'] = folder_toscan
        
        # Sauvegarder dans un fichier config
        with open('config.json', 'w') as f:
            json.dump({'dossier_cible': folder_toscan}, f)
        
        st.success(f"‚úÖ Dossier enregistr√© : {folder_toscan}")
    else:
        st.error("‚ùå Dossier invalide ou inexistant")

# ---------------
# --- SIDEBAR ---
# ---------------

if st.sidebar.button("Initialize database"):
    ROOT_DIR = Path(__file__).resolve().parent
    db_init_path = ROOT_DIR / "working_DB" / "db_init.py"
    subprocess.run(["python", str(db_init_path)], check=True)
    st.success("Script ex√©cut√©!")


if st.sidebar.button("üîç Scan sweep"):
    target = st.session_state.get('dossier_cible')
    if target and Path(target).exists():
        scan_folder_and_store(target, str(DB_PATH))
        st.success("Scan termin√© et base mise √† jour.")
    else:
        st.error("‚ùå Aucun dossier valide s√©lectionn√© pour le scan.")

if st.sidebar.button("üéØ Magic numbers check"):
    st.session_state['selected_tool'] = "Magic numbers check"

if st.sidebar.button("üí∞ Crude benefits"):
    st.session_state['selected_tool'] = "Crude benefits"

if st.sidebar.button("üìâ Benford NNRA"):
    st.session_state['selected_tool'] = "Benford NNRA"

if st.sidebar.button("üìä Volume map"):
    st.session_state['selected_tool'] = "Volume Map"
    
if st.sidebar.button("üìã Populate metadata"):
    st.session_state['selected_tool'] = "Populate metadata"

if st.sidebar.button("üè∑Ô∏è File labelling"):
    st.session_state['selected_tool'] = "File labelling"

if st.sidebar.button("üîç Regex Analytics"):
    st.session_state['selected_tool'] = "Regex Analytics"

if st.sidebar.button("üìä Estimate work"):
    st.session_state['selected_tool'] = "Estimate work"

if st.sidebar.button("üß† LR semantic"):
    st.session_state['selected_tool'] = "LR semantic"

if st.sidebar.button("üéì HL semantic"):
    st.session_state['selected_tool'] = "HL semantic"

if st.sidebar.button("üìù OCR analysis"):
    st.session_state['selected_tool'] = "OCR analysis"

if st.sidebar.button("üìÑ Final report"):
    st.session_state['selected_tool'] = "Final report"

if st.sidebar.button("ü§ñ Classifier"):
    st.session_state['selected_tool'] = "Classifier"

selected_tool = st.session_state.get('selected_tool')


# --------------------------------
# --- SHOW THE SELECTED FOLDER ---
# --------------------------------

if 'dossier_cible' in st.session_state:
    st.info(f"üìÇ Dossier actuel : {st.session_state['dossier_cible']}")

# -------------------------------
# --- MAGIC NUMBERS CHECK UI ---
# -------------------------------

if selected_tool == "Magic numbers check":
    st.header("üéØ Magic numbers check")

    st.write(
        "Ce module scanne la table `file` de la base SQLite "
        "et remplit la colonne `true_extension` √† partir des magic numbers."
    )

    st.code(f"Base utilis√©e : {DB_PATH}", language="bash")

    if st.button("Lancer le scan Magic Numbers"):
        if not DB_PATH.exists():
            st.error(f"‚ùå Base SQLite introuvable : {DB_PATH}")
        else:
            progress_bar = st.progress(0.0)
            status_text = st.empty()
            log_area = st.empty()

            logs = []

            def progress_callback(current, total, file_path, ext, desc, error):
                # Mise √† jour de la barre de progression
                if total:
                    progress_bar.progress(current / total)
                    status_text.text(f"Traitement {current}/{total}")

                # Log texte
                if file_path is not None:
                    if error:
                        msg = f"[{current}/{total}] ERREUR sur {file_path} : {error}"
                    else:
                        msg = f"[{current}/{total}] {file_path} -> {ext} ({desc})"
                    logs.append(msg)
                    # Afficher seulement les derni√®res lignes pour rester lisible
                    log_area.text("\n".join(logs[-20:]))

            # Si les chemins stock√©s dans la colonne `path` sont ABSOLUS, laisse base_dir=None
            run_magic_numbers_on_db(
                db_path=str(DB_PATH),
                base_dir=None,          # ou str(st.session_state['dossier_cible']) si chemins relatifs
                only_missing=True,
                progress_callback=progress_callback,
            )

            st.success("‚úÖ Scan termin√©. La colonne `true_extension` a √©t√© mise √† jour.")

# -------------------------------
# --- CRUDE BENEFITS UI ---
# -------------------------------

if selected_tool == "Crude benefits":
    st.header("üí∞ Crude benefits")

    st.write(
        "Analyse simple des doublons bas√©e sur les hash SHA256 :\n"
        "- Liste CSV des fichiers dupliqu√©s (m√™me hash_sha256)\n"
        "- Estimation de l'espace disque √©conomisable si on ne conserve qu'un seul exemplaire\n"
        "- Nombre total de fichiers qui pourraient √™tre supprim√©s"
    )

    st.code(f"Base utilis√©e : {DB_PATH}", language="bash")

    if st.button("Lancer l'analyse des doublons"):
        if not DB_PATH.exists():
            st.error(f"‚ùå Base SQLite introuvable : {DB_PATH}")
        else:
            try:
                with st.spinner("Analyse des doublons en cours..."):
                    result = analyze_duplicates(str(DB_PATH))

                if result["groups_count"] == 0:
                    st.success("‚úÖ Aucun doublon d√©tect√© (aucun hash_sha256 en double).")
                else:
                    st.success("‚úÖ Analyse termin√©e.")

                    col1, col2, col3 = st.columns(3)
                    col1.metric("Groupes de doublons", result["groups_count"])
                    col2.metric(
                        "Fichiers potentiellement supprimables",
                        result["removable_files_count"]
                    )
                    col3.metric(
                        "Espace √©conomisable (approx.)",
                        result["wasted_human"]
                    )

                    st.download_button(
                        label="üì• T√©l√©charger la liste CSV des doublons",
                        data=result["csv_bytes"],
                        file_name="duplicate_files_sha256.csv",
                        mime="text/csv"
                    )

            except Exception as e:
                st.error(f"Erreur lors de l'analyse : {e}")

# -------------------------------
# --- FORENSIC AUDIT UI ---
# -------------------------------

if selected_tool == "Forensic Audit":
    st.header("üïµÔ∏è Audit Forensique & Anomalies")
    st.write("Analyse heuristique sur 15 indicateurs cl√©s (Spoofing, Timestomping, ZipBombs, Crypto, etc.)")
    
    st.code(f"Base utilis√©e : {DB_PATH}", language="bash")
    
    if st.button("Lancer l'audit complet"):
        if not DB_PATH.exists():
            st.error(f"‚ùå Base SQLite introuvable : {DB_PATH}")
        else:
            with st.spinner("Ex√©cution des algorithmes forensiques en cours..."):
                results = run_forensic_analysis(str(DB_PATH))
            
            if "error" in results:
                st.error(results["error"])
            else:
                st.success("‚úÖ Audit termin√©. R√©sultats d√©taill√©s ci-dessous.")
                
                # Liste pour l'export consolid√©
                export_list = []
                
                # Dictionnaire de mapping pour l'affichage propre
                descriptions = {
                    "spoofing_df": "üö® Extension Spoofing",
                    "timestomping_df": "‚è∞ Timestomping (>24h d√©calage)",
                    "zipbomb_df": "üí£ Zip Bombs / Compression Suspecte",
                    "ghost_files_df": "üëª Fichiers Fant√¥mes (Hash Collision)",
                    "secrets_df": "üîë Secrets Potentiels (Code/Txt)",
                    "encrypted_df": "üîí Fichiers Chiffr√©s / Prot√©g√©s",
                    "unsigned_exe_df": "‚ö†Ô∏è Ex√©cutables Non Sign√©s",
                    "gdpr_heatmap_df": "üõ°Ô∏è Densit√© RGPD (Par dossier)",
                    "silent_hours_df": "üåô Activit√© Suspecte (Nuit/WE)",
                    "authors_df": "‚úçÔ∏è Auteurs Externes / Multiples",
                    "fakework_df": "‚ö° Fake Work / G√©n√©ration Rapide",
                    "cameras_df": "üì∑ Empreinte Mat√©rielle (Appareils)",
                    "zombies_df": "üßü Fichiers Zombies (>3 ans)",
                    "tech_debt_df": "üèöÔ∏è Dette Technique (Code)",
                    "geo_df": "üåç Dispersion G√©ographique"
                }

                # Affichage des r√©sultats non vides
                count_anomalies = 0
                
                for key, title in descriptions.items():
                    df = results.get(key)
                    if df is not None and not df.empty:
                        count_anomalies += len(df)
                        with st.expander(f"{title} ({len(df)} √©l√©ments)", expanded=False):
                            st.dataframe(df)
                        
                        # Pr√©paration Export : On standardise pour concat√©ner
                        df_export = df.copy()
                        df_export.insert(0, 'Anomaly_Type', title)
                        # On convertit tout en string pour √©viter les conflits de types lors du merge
                        df_export = df_export.astype(str)
                        export_list.append(df_export)
                
                if count_anomalies == 0:
                    st.info("Aucune anomalie d√©tect√©e sur l'ensemble des indicateurs.")
                else:
                    st.warning(f"Total : {count_anomalies} anomalies ou points d'attention d√©tect√©s.")

                # Bouton Export CSV Unifi√©
                if export_list:
                    full_report = pd.concat(export_list, ignore_index=True)
                    
                    # R√©organisation intelligente des colonnes pour l'export
                    cols = list(full_report.columns)
                    # On met Anomaly et path au d√©but si existants
                    if 'Anomaly_Type' in cols:
                        cols.insert(0, cols.pop(cols.index('Anomaly_Type')))
                    if 'path' in cols:
                        cols.insert(1, cols.pop(cols.index('path')))
                    
                    full_report = full_report[cols]
                    
                    csv_data = full_report.to_csv(index=False).encode('utf-8')
                    
                    st.download_button(
                        label="üì• T√©l√©charger le Rapport d'Anomalies (CSV)",
                        data=csv_data,
                        file_name="rapport_forensic_complet.csv",
                        mime="text/csv"
                    )



# -------------------------------
# --- POPULATE METADATA UI ---
# -------------------------------

if selected_tool == "Populate metadata":
    st.header("üìã Extraction des M√©tadonn√©es")
    st.write(
        "Ce module parcourt tous les fichiers index√©s et extrait les m√©tadonn√©es techniques "
        "(EXIF, ID3, propri√©t√©s Office, stats Code, etc.) selon leur type identifi√©."
    )

    if st.button("Lancer l'extraction"):
        if not DB_PATH.exists():
            st.error("‚ùå Base de donn√©es introuvable. Veuillez lancer l'initialisation et le scan d'abord.")
        else:
            progress_bar = st.progress(0.0)
            status_text = st.empty()
            log_area = st.empty()
            logs = []

            def meta_callback(current, total, filename, status):
                if total > 0:
                    progress_bar.progress(current / total)
                    status_text.text(f"Traitement {current}/{total} : {filename}")
                
                # On log seulement les succ√®s/erreurs, pas les SKIPPED pour r√©duire le bruit
                if "SUCCESS" in status:
                    logs.append(f"‚úÖ {filename} : {status}")
                elif "ERROR" in status:
                    logs.append(f"‚ùå {filename} : {status}")
                
                # Affiche les 15 derni√®res lignes
                if logs:
                    log_area.text("\n".join(logs[-15:]))

            run_global_metadata_population(
                db_path=str(DB_PATH),
                progress_callback=meta_callback
            )
            
            st.success("‚úÖ Extraction des m√©tadonn√©es termin√©e.")


# -------------------------------
# --- BENFORD ANALYSIS UI ---
# -------------------------------

if selected_tool == "Benford NNRA":
    st.header("üìâ Benford Natural Number Analysis")
    st.write(
        "Cette analyse v√©rifie si la distribution des tailles de fichiers suit la loi de Benford (sur les 2 premiers chiffres). "
        "Une d√©viation significative peut indiquer des donn√©es g√©n√©r√©es artificiellement, chiffr√©es ou alt√©r√©es."
    )
    
    st.code(f"Base utilis√©e : {DB_PATH}", language="bash")

    if st.button("Lancer l'analyse Benford"):
        if not DB_PATH.exists():
            st.error(f"‚ùå Base SQLite introuvable : {DB_PATH}")
        else:
            with st.spinner("Calcul des distributions en cours..."):
                res = analyze_benford_distribution(str(DB_PATH))
            
            if res["success"]:
                st.success("Analyse termin√©e.")
                
                # 1. Metrics
                col1, col2 = st.columns(2)
                col1.metric("Fichiers analys√©s", res["file_count"])
                col1.metric("Score Chi-Carr√©", f"{res['chi_square']:.2f}")
                
                # Interpr√©tation visuelle
                if "‚úÖ" in res["interpretation"]:
                    col2.success(res["interpretation"])
                elif "‚ö†Ô∏è" in res["interpretation"]:
                    col2.warning(res["interpretation"])
                else:
                    col2.error(res["interpretation"])
                
                # 2. Graphique
                st.pyplot(res["fig"])
                
                # 3. Data Expander
                with st.expander("Voir les donn√©es brutes"):
                    st.dataframe(res["dataframe"])
            else:
                st.error(f"Erreur : {res['error']}")

# VOLUMETRIC MAP

if selected_tool == "Volume Map":
    st.header("üìä Cartographie volum√©trique")

    if st.button("Calculer la cartographie"):
        df = get_folder_volume_df(str(DB_PATH))

        st.write("Top 30 dossiers par volume :")
        df_top = df.sort_values("total_size_bytes", ascending=False).head(30)
        df_top_display = df_top.assign(
            total_size_gb = df_top["total_size_bytes"] / (1024**3)
        )
        st.dataframe(df_top_display[["folder_path", "file_count", "total_size_gb"]])

        # Export CSV
        csv_bytes = df.to_csv(index=False).encode("utf-8")
        st.download_button(
            label="üì• T√©l√©charger le CSV complet",
            data=csv_bytes,
            file_name="volume_map.csv",
            mime="text/csv",
        )

# -------------------------------
# --- REGEX ANALYTICS UI ---
# -------------------------------

if selected_tool == "Regex Analytics":
    st.header("üîç Analyse Regex Forensique (16 cat√©gories)")

    st.write(
        "Ce module scanne tous les fichiers texte (code, logs, configs, etc.) "
        "√† la recherche de donn√©es sensibles comme :\n"
        "- NSS, Carte d‚Äôidentit√©, T√©l√©phone\n"
        "- Mot de passe, Cl√© API, Carte bancaire\n"
        "- URL internes, Fichiers temporaires, Commentaires sensibles\n"
        "\n"
        "Les r√©sultats sont enregistr√©s dans la base et affich√©s ci-dessous."
    )

    st.code(f"Base utilis√©e : {DB_PATH}", language="bash")

    if st.button("Lancer l'analyse Regex"):
        with st.spinner("Analyse en cours... (16 cat√©gories de regex)"):
            try:
                # Importer le module de d√©tection
                from src.forensic_detector import run_forensic_scan

                # Ex√©cuter le scan
                run_forensic_scan(str(DB_PATH))

                # R√©cup√©rer les r√©sultats depuis la base
                conn = sqlite3.connect(str(DB_PATH))
                df = pd.read_sql_query("""
                    SELECT 
                        f.path AS fichier,
                        d.category AS cat√©gorie,
                        d.value AS valeur,
                        d.detected_at AS date_detection
                    FROM file_sensitivity_detection d
                    JOIN file f ON d.file_id = f.id
                    ORDER BY d.detected_at DESC
                """, conn)
                conn.close()

                # Afficher le tableau
                st.success(f"‚úÖ Analyse termin√©e. {len(df)} d√©tections trouv√©es.")

                if not df.empty:
                    st.dataframe(df)

                    # Export CSV
                    csv_data = df.to_csv(index=False).encode('utf-8')
                    st.download_button(
                        label="üì• T√©l√©charger les r√©sultats (CSV)",
                        data=csv_data,
                        file_name="regex_detection_results.csv",
                        mime="text/csv"
                    )

                    # Statistiques par cat√©gorie
                    st.subheader("üìä R√©sum√© par cat√©gorie")
                    stats = df['cat√©gorie'].value_counts()
                    st.bar_chart(stats)

                else:
                    st.info("Aucune donn√©e sensible d√©tect√©e.")

            except Exception as e:
                st.error(f"‚ùå Erreur lors de l'analyse : {e}")